# -*- coding: utf-8 -*-
"""Cluster Analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tex6VkPJmsbf2sv5mrofeGHWHmYSzz16
"""

# mounting to google drive
from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans

# reading csv into pandas dataframes
dataset= pd.read_csv('drive/My Drive/Colab Notebooks/data.csv')

X=dataset[["data_a","data_b","data_c","data_d","data_e","data_f","data_g","data_h","data_i","data_j"]]

X

# Remove row with index 92
X = X.drop(index=92).reset_index(drop=True)

X

from yellowbrick.cluster import KElbowVisualizer
from sklearn.cluster import KMeans

model = KMeans(random_state=42)
visualizer = KElbowVisualizer(model,k=(2,10), timings=True)
visualizer.fit(X)
visualizer.show()
      # Finalize and render figure

from sklearn.metrics import silhouette_samples, silhouette_score, davies_bouldin_score, calinski_harabasz_score

from sklearn.cluster import KMeans

kmeans = KMeans(n_clusters=5, random_state=0)

kmeans.fit(X)
KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,
       n_clusters=5, n_init=10,
       random_state=0, tol=0.0001, verbose=0)

kmeans.labels_

kmeans_cluster_labels = kmeans.fit_predict(X)

from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score

print("Silhouette Score:", silhouette_score(X, kmeans_cluster_labels))
print("Calinski-Harabasz Index:", calinski_harabasz_score(X, kmeans_cluster_labels))
print("Davies-Bouldin Index:", davies_bouldin_score(X, kmeans_cluster_labels))
print("Inertia (SSE):", kmeans.inertia_)

y_pred = kmeans.predict(X)

from scipy.spatial import distance

cluster_labels = kmeans.fit_predict(X)
# Example of calculating Dunn's Index (DI)
def intra_cluster_distance(cluster):
    # Calculate pairwise distances within the cluster
    return np.mean(distance.pdist(cluster))

def inter_cluster_distance(centroids):
    # Calculate pairwise distances between centroids
    min_inter_cluster_distance = np.inf
    for i, centroid1 in enumerate(centroids):
        for j, centroid2 in enumerate(centroids):
            if i != j:
                dist = distance.euclidean(centroid1, centroid2)
                if dist < min_inter_cluster_distance:
                    min_inter_cluster_distance = dist
    return min_inter_cluster_distance

# Assuming 'cluster_labels' contain the cluster labels assigned by the clustering algorithm
clusters = []
for cluster_label in np.unique(cluster_labels):
    clusters.append(X[cluster_labels == cluster_label])

intra_distances = [intra_cluster_distance(cluster) for cluster in clusters]
inter_distance = inter_cluster_distance(kmeans.cluster_centers_)

dunn_index = min(intra_distances) / inter_distance
print("Dunn's Index (DI):", dunn_index)

cluster_map = pd.DataFrame()
cluster_map['data_index'] = X.index.values  # use original DataFrame's index
cluster_map['cluster'] = kmeans.labels_

kmeans_cluster0=cluster_map[cluster_map.cluster == 0]
kmeans_cluster1=cluster_map[cluster_map.cluster == 1]
kmeans_cluster2=cluster_map[cluster_map.cluster == 2]

!pip install scikit-learn-extra

!pip install numpy==1.26.4

"""## **KMedoids**"""

from sklearn_extra.cluster import KMedoids

kmedoids = KMedoids(n_clusters=5)
pam=kmedoids.fit(X)
#kmedoids_labels=kmedoids.fit_predict(X_scaled)
y_kmedoids_pred=kmedoids.fit_predict(X)

labels=kmedoids.labels_

print("Silhouette Score:", silhouette_score(X, labels))
print("Calinski-Harabasz Index:", calinski_harabasz_score(X, labels))
print("Davies-Bouldin Index:", davies_bouldin_score(X, labels))
print("Inertia (SSE):", kmedoids.inertia_)

"""# Fuzzy Clustering"""

!pip install fuzzy-c-means

from fcmeans import FCM
fuzzy_c = FCM(n_clusters=5) # we use two cluster as an example
fuzzy_c_model=fuzzy_c.fit(X.to_numpy())## X, numpy arra

y_fuzzy_c_pred=fuzzy_c.predict(X.to_numpy())

print("Silhouette Score:", silhouette_score(X, y_fuzzy_c_pred))
print("Calinski-Harabasz Index:", calinski_harabasz_score(X, y_fuzzy_c_pred))
print("Davies-Bouldin Index:", davies_bouldin_score(X, y_fuzzy_c_pred))
#print("Inertia (SSE):", kmedoids.inertia_)

"""# Hierarchchical CLustering

**wards**
"""

from sklearn.cluster import AgglomerativeClustering
clusterwards = AgglomerativeClustering(n_clusters=5, linkage='ward')
y_wards_pred=clusterwards.fit_predict(X)

print("Silhouette Score:", silhouette_score(X, y_wards_pred))
print("Calinski-Harabasz Index:", calinski_harabasz_score(X, y_wards_pred))
print("Davies-Bouldin Index:", davies_bouldin_score(X, y_wards_pred))
#print("Inertia (SSE):", clusterwards.inertia_)

"""**single**"""

from sklearn.cluster import AgglomerativeClustering
clustersingle = AgglomerativeClustering(n_clusters=5,linkage='single')
y_single_pred=clustersingle.fit_predict(X)

print("Silhouette Score:", silhouette_score(X, y_single_pred))
print("Calinski-Harabasz Index:", calinski_harabasz_score(X, y_single_pred))
print("Davies-Bouldin Index:", davies_bouldin_score(X, y_single_pred))
#print("Inertia (SSE):", clusterwards.inertia_)

"""**complete**"""

from sklearn.cluster import AgglomerativeClustering
clustercomplete = AgglomerativeClustering(n_clusters=5, linkage='complete')
y_complete_pred=clustercomplete.fit_predict(X)

print("Silhouette Score:", silhouette_score(X, y_complete_pred))
print("Calinski-Harabasz Index:", calinski_harabasz_score(X, y_complete_pred))
print("Davies-Bouldin Index:", davies_bouldin_score(X, y_complete_pred))

"""**average**"""

from sklearn.cluster import AgglomerativeClustering
cluster = AgglomerativeClustering(n_clusters=5, linkage='average')
y_average_pred=cluster.fit_predict(X)

print("Silhouette Score:", silhouette_score(X, y_average_pred))
print("Calinski-Harabasz Index:", calinski_harabasz_score(X, y_average_pred))
print("Davies-Bouldin Index:", davies_bouldin_score(X, y_average_pred))

"""# DBSCAN"""

!pip install kneed

from sklearn.cluster import DBSCAN

# Instantiate DBSCAN
dbscan = DBSCAN(eps=0.5, min_samples=5)  # adjust these based on your data

# Fit and predict cluster labels
y_dbscan_pred = dbscan.fit_predict(X)

print("Silhouette Score:", silhouette_score(X, y_dbscan_pred))
print("Calinski-Harabasz Index:", calinski_harabasz_score(X, y_dbscan_pred))
print("Davies-Bouldin Index:", davies_bouldin_score(X, y_dbscan_pred))

"""## **HDBSCAN**"""

!pip install hdbscan

import hdbscan

# Instantiate HDBSCAN
hdb = hdbscan.HDBSCAN(min_cluster_size=5)  # You can tune this

# Fit and predict cluster labels
y_hdbscan_pred = hdb.fit_predict(X)

print("Silhouette Score:", silhouette_score(X, y_hdbscan_pred))
print("Calinski-Harabasz Index:", calinski_harabasz_score(X, y_hdbscan_pred))
print("Davies-Bouldin Index:", davies_bouldin_score(X, y_hdbscan_pred))

"""## **OPTICS**"""

from sklearn.cluster import OPTICS

# Instantiate OPTICS
optics = OPTICS(min_samples=5, xi=0.05, min_cluster_size=0.05)  # tweakable

# Fit the model and predict cluster labels
y_optics_pred = optics.fit_predict(X)

print("Silhouette Score:", silhouette_score(X, y_optics_pred))
print("Calinski-Harabasz Index:", calinski_harabasz_score(X, y_optics_pred))
print("Davies-Bouldin Index:", davies_bouldin_score(X, y_optics_pred))

"""# Gausian Mixture Models"""

from pandas import DataFrame
from sklearn.preprocessing import StandardScaler, normalize
from sklearn.decomposition import PCA
from sklearn.mixture import GaussianMixture
from sklearn.metrics import silhouette_score
from sklearn.model_selection import train_test_split
from sklearn import metrics

gmm = GaussianMixture(n_components = 5)
gmm.fit(X)
y_gmm_pred=gmm.fit_predict(X)

print("Silhouette Score:", silhouette_score(X, y_gmm_pred))
print("Calinski-Harabasz Index:", calinski_harabasz_score(X, y_gmm_pred))
print("Davies-Bouldin Index:", davies_bouldin_score(X, y_gmm_pred))

"""# spectral"""

from sklearn.cluster import SpectralClustering

clustering = SpectralClustering(n_clusters=5,assign_labels='discretize', random_state=0)

clustering.fit(X)

pred=clustering.fit_predict(X)

print("Silhouette Score:", silhouette_score(X, pred))
print("Calinski-Harabasz Index:", calinski_harabasz_score(X, pred))
print("Davies-Bouldin Index:", davies_bouldin_score(X, pred))

"""# Birch"""

from sklearn.cluster import Birch

brc=Birch(n_clusters=5)
brc

birch_model=brc.fit(X)

y_brc_pred=brc.predict(X)

print("Silhouette Score:", silhouette_score(X, y_brc_pred))
print("Calinski-Harabasz Index:", calinski_harabasz_score(X, y_brc_pred))
print("Davies-Bouldin Index:", davies_bouldin_score(X, y_brc_pred))

"""## **Best Model**"""

# Create a list to hold the results
results = []

# Append results as [method, silhouette, calinski, davies]
results.append(['KMeans', silhouette_score(X, kmeans_cluster_labels), calinski_harabasz_score(X, kmeans_cluster_labels), davies_bouldin_score(X, kmeans_cluster_labels)])
results.append(['KMedoids', silhouette_score(X, labels), calinski_harabasz_score(X, labels), davies_bouldin_score(X, labels)])
results.append(['Fuzzy C-Means', silhouette_score(X, y_fuzzy_c_pred), calinski_harabasz_score(X, y_fuzzy_c_pred), davies_bouldin_score(X, y_fuzzy_c_pred)])
results.append(['Agglomerative - Ward', silhouette_score(X, y_wards_pred), calinski_harabasz_score(X, y_wards_pred), davies_bouldin_score(X, y_wards_pred)])
results.append(['Agglomerative - Single', silhouette_score(X, y_single_pred), calinski_harabasz_score(X, y_single_pred), davies_bouldin_score(X, y_single_pred)])
results.append(['Agglomerative - Complete', silhouette_score(X, y_complete_pred), calinski_harabasz_score(X, y_complete_pred), davies_bouldin_score(X, y_complete_pred)])
results.append(['Agglomerative - Average', silhouette_score(X, y_average_pred), calinski_harabasz_score(X, y_average_pred), davies_bouldin_score(X, y_average_pred)])

# Check for noise-only clustering in DBSCAN, HDBSCAN, and OPTICS
def safe_metrics(method_name, labels):
    if len(set(labels)) <= 1 or -1 in set(labels) and len(set(labels)) == 2:
        return [method_name, None, None, None]
    return [method_name, silhouette_score(X, labels), calinski_harabasz_score(X, labels), davies_bouldin_score(X, labels)]

results.append(safe_metrics('DBSCAN', y_dbscan_pred))
results.append(safe_metrics('HDBSCAN', y_hdbscan_pred))
results.append(safe_metrics('OPTICS', y_optics_pred))

results.append(['GMM', silhouette_score(X, y_gmm_pred), calinski_harabasz_score(X, y_gmm_pred), davies_bouldin_score(X, y_gmm_pred)])
results.append(['Spectral Clustering', silhouette_score(X, pred), calinski_harabasz_score(X, pred), davies_bouldin_score(X, pred)])
results.append(['Birch', silhouette_score(X, y_brc_pred), calinski_harabasz_score(X, y_brc_pred), davies_bouldin_score(X, y_brc_pred)])

# Convert to DataFrame
results_df = pd.DataFrame(results, columns=['Method', 'Silhouette Score', 'Calinski-Harabasz Index', 'Davies-Bouldin Index'])

# Display the table
print(results_df)

# Optionally, sort by silhouette score (descending)
results_df.sort_values(by='Silhouette Score', ascending=False)

cluster_map = pd.DataFrame()
cluster_map['data_index'] = X.index.values
cluster_map['cluster'] = y_average_pred

# Extract clusters
avg_cluster0 = cluster_map[cluster_map.cluster == 0]
avg_cluster1 = cluster_map[cluster_map.cluster == 1]
avg_cluster2 = cluster_map[cluster_map.cluster == 2]
avg_cluster3 = cluster_map[cluster_map.cluster == 3]
avg_cluster4 = cluster_map[cluster_map.cluster == 4]

avg_cluster0

avg_cluster1

avg_cluster2

avg_cluster3

avg_cluster4

